# The evolving patterns of Knowledge on Wikipedia: Good intentions, Bad Actors and computational means of distinguishing the two

# setting the stage - what is wikipedia

- how does it work - the beauty of the public edits
- why was it researched - the wealth of language knowkedge
- the API
  > todo: examples of great wikipedia research in few domains
- the raise of the automatic research methods: enganglement in LLM datasets.
  > due to it being neutrally formattd

# the secondary process: weaponizing the common ground

## world we live in

- sheer amount of data, tsunami of user-generated content
- the exaples of misinformation campaigns: Cambridge Analityca
- info on the contract from US Government for deep fake actors creation

## wiki as target of malicious actors

Easy Pray

- wikipedia is open, meaning anyone can edit articles
- start with the silly ones - Batuty street, **find some other spoofs**

Tasty Treat

- wikipedia is "source of truth", used in education
- the serious ones: Wiki.HR
  **context** - a project divided, a project captured
  **methods** - how exactly was the bias spread?
  **scale** - number of editions, timespan and estimated influence

# Perspectives for protecting the public good

## Why automatic?

**mention of Wikipedia's scale and governance and feasability of quantitative, human-driven assessment of changes**

## the troubles with automatic detection

1. theory - common definition of unwanted changes
2. practice - why flag (and not block - human in the loop), what to flag

## what Wikipedia currently uses

[!!!]**describe the models**
[!!!]**their shortcomings**

## the future - could LLMs help us monitor revisions?

**LLM features that make them useful in research**

## existing methods **WIKIPEDIA - CURRENT METHODS**

_Abstract_
The talk offers an end-to-end perspective into Wikipedia's article revision process and the challenges it faces, especially in the advent of Generative AI. We will set up the stage with a primer into the platform itself and the features that make is so important in natural language processing (NLP). Useful as it is, Wikipedia's founding assumptions bring certain risks to the table, so the aim is to get informed about these. We will deep dive into the harm that's already been done (and fixed!) in the past, to get a better grip on the "how's" and "why's". Ultimately, we will ask and attempt to try to answer some of the big questions regarding ways of protecting the common good from ill will. Why would we even turn automatic? What systems already protect us from malicious edits? Can we do more with Large Language Models?

To get a more detailed view iFirstly, we will take a closer look into the platform itself and assess the risks some of its ounding assumptions bring. To better understand these risks, we will put the platofrm mechanics and governance in context and discuss some of disinformation campaigns conducted in the past.

perspective into the world of Natural Language Processing research on Wikipedia. It aims to presen the entaglement and friction it introduces its relation with Wikipedia into computational research an intro into Wikipedia research and the most commonly applied computational mechanisms to prevent vandalism and manipulation attempts. presents a landscape of computation means utilised in fighting against auto-generated
