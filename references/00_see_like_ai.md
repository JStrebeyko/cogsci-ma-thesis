> LLMs struggled with bias detection, achieving only 64% accuracy on

> high-recall but low-precision editing.

> To steer models towards specific norms and values, there is a growing trend of stating high-level rules as prompts. For

> mirrors earlier debates in human-AI interaction and beyond. For instance,Lucy Suchman’s 1987 “Plans and Situated Actions” frameworkcontrasted predetermined procedures derived from universal principles (“plans”) with context-dependent actions based on concrete circumstances (“situated actions”). She argued that AI systems execute plans, while humans perform situated actions. This distinction relates to James Scott’s [41] concept of _“seeing like a state”_ — the idea that the large-scale plans of centralized authorities often break down when faced with complex, local realities.

### NPOV is difficult in practice

> The broader NPOV policy encompasses various complex assessments (e.g., covering viewpoints in proportion to their prominence in reliable sources).

### reasons

> Evaluating LLMs on Wikipedia’s NPOV policy is an interesting test case of model abilities for three reasons.

interesting, because

1. LLMs used in very different contexts
2. non-neutrality is nuanced, goes beyond literal meaning
3. tension between too easy it is to articulate N Pov and how complex it is to implement
