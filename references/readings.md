### The Case of Croatian Wikipedia: Encyclopaedia of Knowledge or Encyclopaedia for the Nation?

> This report represents the evaluation of the Croatian disinformation case by an external expert on the subject matter,

> (...) Serbo-Croatian Wikipedia’s community was split up into Croatian, Bosnian, Serbian, and the original Serbo-Croatian wikis starting in 2003. The report concludes that this structure enabled local language communities to sort by points of view on each project, often falling along political party lines in the respective regions.

> a group of administrators and editors have held de-facto control over the project for more than a decade.

> During that time, evidence that he evaluated suggests that they have intentionally distorted the content presented in articles, abused power, and systematically obstructed otherwise accepted global Wikipedia community practices.

> the group leaders who were primarily responsible for abusing the project through sock-puppeting.

> proven that a core member of the group was obstructing community voting procedures through "sock-puppets", a highly manipulative practice of creating and using inauthentic online identities for the purpose of deception.

> revealed a widespread pattern of manipulative behaviour and abuse of power by an ideologically aligned group of Croatian language Wikipedia (Hr.WP) admins and other volunteer editors.

editors banned in 2020

> It lasted more than ten years and has led to the departure of editors upholding the five pillars and profound distortion of content in hundreds of articles and across different topics. Project capture of Croatian language Wikipedia has exposed – and exploited – a weakness in Wikipedia’s model of community self-governance. (this case) has demonstrated that both community and content can and will decline if institutions are taken over by an organised and ideologically aligned group.

project capture, radical right grouping, systematic obstruction of traditional community processes, mission misalignment

> this group consisted of real-life friends, ideological sympathisers and political allies
> The group has been using its on-wiki positions of power to attract new like-minded contributors, silence and ban dissenters, manipulate community elections and subvert Wikipedia’s and the broader movement’s native conflict resolution mechanisms.

> obstruction was the result of a well-organised effort by the same closely connected group. It further appears that the group intentionally deflected legitimate concerns about content bias and/or problematic behaviour by using well-known disinformation tactics, including relativisation of facts, whataboutism, discreditation of other participants and outright bullying. Contrary to what might have been expected, the group of misaligned Hr.WP admins actively participated in Meta RfC discussions.

> this evaluation has found that the quality of content on Hr.WP degraded progressively between 2013 and 2019, with more revisionist claims and disinformation inserted in an increasing number of articles each year.

> The experience of Croatian language Wikipedia raises important questions regarding global governance risks. It challenges the widely held view that all of Wikipedia is fairly and uniformly resilient in withstanding organised disinformation campaigns aiming to capture its language versions.

> Generally, the existing academic research into attempts at mounting organised disinformation campaigns on Wikipedia has primarily focused on detecting edits' frequency and filtering out suspicious editors
> (our niche)

> The lessons from Hr.WP pave the way for active community monitoring.

> Fresh calls for more institutional scrutiny of [other platforms hosting user-generated content] other platforms now come from established democracies trying to design an appropriate response to a string of coordinated disinformation campaigns that destabilised their election processes and undermined civil society.

> Seen by many governments as accomplices – if not outright enablers – of disinformation operations, major social media platforms will likely become the main targets of new regulatory legislation.

> The case of Croatian language Wikipedia demonstrates that there could be similar attempts of project capture in other languages as well.
> (global risk)

> A more resourced and better-organized attempt could be harder to detect and eventually reverse.

> Comparative analysis of the content published on the Croatian language Wikipedia against other language projects (...) revealed numerous examples of systemic, deeply-rooted bias and disinformation

> The analysis (...) confirmed that they had for years contained pervasive ideological bias.

Biases:

- Framing ("Also known as emulated neutrality. Giving equal weight to a range of competing claims, presenting questionable claims side by side with factual truths, or contextualising facts in a way that misleads readers.")
- source ("Citing non-neutral sources without disclosing their affiliation and/or providing dubious sources to back up claims presented in the article." framing bias often relies on it)
- selection ("Selectively including and excluding content regardless of its notability or topical relevance")

> Identifying disinformation, which usually entails demonstrating intent, on Wikipedia as such is substantially more difficult.

> In larger language Wikipedia projects, like English, Spanish, French or German, this diversity comes from the sheer number of participants, but also from the fact that these are all pluricentric languages with substantial numbers of native speakers coming from different countries, bringing somewhat different socio-political and cultural frames of reference to the well-established editorial peer on-wiki processes.
> (PL more at risk)

what it takes for disinformation campaign:

> A possible way to implement a disinformation campaign would be for an organised group of user accounts to take over the entire language project by banning dissenters and "installing" ideologically-aligned user accounts in roles essential to self-governance processes. But for this effort to have any influence on a wider society reliant on the Wikipedia language project, it has to impact content in a substantial number of articles and for an extended period of time.

1. organized group at lead positions
2. many articles for extended period of time

> In the case of Croatian language Wikipedia, articles affected by historical revisionism and manipulation of
> facts have been online and influencing the public sphere for more than a decade.
> [against the ideals of five pillars to be upheld in all Wikipedia projects], the ultimate goal of this endeavour appears to be to try to _influence the reader’s final moral or value judgement of a person, phenomenon or historical event_ described in an article and to do so in a way that supports and corresponds to the influencer’s broader ideological views. (emphasis mine) (out of alignment with first and second pillar)

Question: can we use moral judgement as a metric?

#### recommendation

(encouraging building local governance)
for:

> However, the process of Hr.WP’s realignment has just begun, and the community may need to consider designing and developing instruments and institutions that will allow for more efficient content oversight, communication, and exchange of resources

against:

> The solutions implemented by the community must be sustainable. To ensure this, it is advisable that the community puts in place a monitoring committee

So content monitoring is needed regardless

#### ADDITIONAL OBSERVATION: Strengthen global governance

> It might also be worthwhile for the community or the foundation to investigate whether technical solutions could have exposed the misconduct of a group of users sooner.

> The communities and the foundation could also engage in a dialogue and develop collaborative solutions that would help monitor communities globally to timely identify major ideological shifts in projects.

thesis project aim is to address this need

#### measuring disinformation
