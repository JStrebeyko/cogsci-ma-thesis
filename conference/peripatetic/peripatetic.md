# Dynamics at the verge of Knowledge: good intentions, bad actors and computational means of distinguishing the two on Wikipedia

# setting the stage - what is wikipedia

- how does it work - the beauty of the public edits
- why was it researched - the wealth of language knowkedge
- the API
  > todo: examples of great wikipedia research in few domains
- the raise of the automatic research methods: enganglement in LLM datasets.
  > due to it being neutrally formattd

# the secondary process: weaponizing the common ground

## world we live in

- sheer amount of data, tsunami of user-generated content
- the exaples of misinformation campaigns: Cambridge Analityca
- info on the contract from US Government for deep fake actors creation

## wiki as target of malicious actors

Easy Pray

- wikipedia is open, meaning anyone can edit articles
- start with the silly ones - Batuty street, **find some other spoofs**

Tasty Treat

- wikipedia is "source of truth", used in education
- the serious ones: Wiki.HR
  **context** - a project divided, a project captured
  **methods** - how exactly was the bias spread?
  **scale** - number of editions, timespan and estimated influence

# Perspectives for protecting the public good

## Why automatic?

**mention of Wikipedia's scale and governance and feasability of quantitative, human-driven assessment of changes**

## the troubles with automatic detection

1. theory - common definition of unwanted changes
2. practice - why flag (and not block - human in the loop), what to flag

## what Wikipedia currently uses

[!!!]**describe the models**
[!!!]**their shortcomings**

## the future - could LLMs help us monitor revisions?

**LLM features that make them useful in research**

## existing methods **WIKIPEDIA - CURRENT METHODS**

_Abstract_
The talk offers an end-to-end perspective into Wikipedia's article revision process and the challenges it faces, especially in the advent of Generative AI. We will set up the stage with a primer into the platform's features that make is so valuable for natural language processing (NLP) needs. Useful as it is, though, Wikipedia's founding assumptions bring certain risks to the table we need to address. To get a better grip on the "how's" and "why's", we will deep dive into the harm that's already been done (and fixed!) in the past. What connects House Representatives, Indian Enterprenours and the right-wing bias in Croatian politics? With our imagination stimulated, we will examine the tools Wikimedia Foundation has at its disposal to flag ill will and ward off malicious revisions, vandalism and manipulation attempts. Which quirks of the platform make these efforts insufficient? The future is automatic, so our eyes will turn towards the next big thing. In search of neutrality guardrails, preventing us from drifting too far-off, we will explore the ways in which Large Language Models can aid the case. What's their promise? What could go wrong?

To get a more detailed view iFirstly, we will take a closer look into the platform itself and assess the risks some of its ounding assumptions bring. To better understand these risks, we will put the platofrm mechanics and governance in context and discuss some of disinformation campaigns conducted in the past.

perspective into the world of Natural Language Processing research on Wikipedia. It aims to presen the entaglement and friction it introduces its relation with Wikipedia into computational research an intro into Wikipedia research and the most commonly applied computational mechanisms to prevent vandalism and manipulation attempts. presents a landscape of computation means utilised in fighting against auto-generated
