# AZ

## co zrobiłem

### lokalny dewelopment

- Infra gotowa w/ ollama - lokalny model Llama 3.2 3B stoi, umie wypluwać rzeczy do bazy danych;

- wejście w rewizje polsko-ukraińskie

### wikipedia research

- Potwierdzenie potrzeby - duży wgląd w MLową infrę Wikipedii, potwierdzenie sensowności stworzenia "czulszego" narzędzia;

- reachout - Piotr Przybyła, PhD (Uni Pompeu Fabra) dał ważny feedback

- "klasyczna" analiza ilościowa - czy da się zaobserwować wzmorzony ruch w edycjach w pewnej grupie powiązanych z PL / UA artykułów?

### problemy z oryginalnym projektem

- detekcja intencji "klasycznie trudna"
- brak benchmarku, ORES działa inaczej niż myślałem
- datasety po polsku - brak granularnych, npov-related. produkcja własnego - odradzona (experci dziedzinowi, pare tysięcy olabelowaych przykładów, najlepiej dwóch annotatorów, najlepiej - biegłych w ukraińskim)
- llmy klasycznie słabe w to zadanie (~61%)

## pivot

- uleszenie detekcji LLM - są papery które skupione na "neutralizacji", gdzie część "detekcyjna" dot. subiektywności ma się słąbo. Propozycje autorów
- dla angielskiego istnieje dataset (WNC, potem WikiBias), pozwalający benchmarkować

👉🏼 model "heat-mapujący" artykuły jeśli chodzi o prawdopodobieństwo zmian;

👉🏼 zbenchmarkowanie modeli głównie anglo-języcznych (LLama / ChatGPT) i polskich (PLLuM / Bielik) na zadaniach związanych z SA / NER / stance detection;

👉🏼 analiza komparatywna "trudnych" tematów UA <-> PL z użyciem bardziej tradycyjnych narzędzi NLP - SA, NER, lub network analysis - Temporal Networks czy jeszcze innych, jak "informational gaps" i szeregi czasowe, supplementowane np porównaniem zmian, które zostały zrevertowanych i nie (np czy wprowadzili je zalogowani użytkownicy czy nie) itd.;

## notatki

### Pomysł 1: Pivot

- szybko, ale nudne

### Pomysł 2: Wojny edycyjne

"co jest clue zmiany edycyjnej?" "czy to szczegół", "czy to ważne"
"jeżeli charakter edycji idzie w stronę detalu, może to ingerencja"

- weź jakiekolwiek edycje, zapytać "czym różni się ta edycja" - "jak to się zmienia"? "na ile sens artykułu się zmienia?", "na ile sens artykułu"
- robisz embeddingi, clastring //
- ogólny problem jest trudny, ale tutaj taki insight

### ⭐ Pomysł 3: pierwszy akapit wikipedii

- wpleść elementy o chorwacji
- założenie: duże walki o pierwszy akapit, sugeruje bardzo dużo
- jak pierwszy akapit się różni między "dobrymi" a "złymi"?
- wymień ważne różnice faktograficzne między pierwszym akapitem a skompresowaną róznicę. Czy jest jakaś zależność między osią / aliantami?

1. 100 postaci historycznych najbardziej znane (WWII)
2. weź NAJTAŃSZY MODEL (GPT 3.5) ewentualnie polskiego bielika, LLamę
3. bierzesz pierwszy akapit, każesz faktograficznie skompresować resztę (max 300 słów), "podsumuj biogram bazując wyłącznie na tekście"
4. przesyłasz pierwszy akapit i skompresowany artykuł, pytasz: czym się różnią, które są zawarte, a które pominięte? Pozytywne / negatywne? w czterech punktach (do 300-słów) wymień co zawarte, co pominięte, "BAZUJĄC NA TEKSCIE"
5. embeddingi tych 4 punktów dla każdej z postaci
6. klastrujesz
7. czy w tej różnicy doboru faktów historiograficznych dla postaci da się wyczuć z której strony pochodzą?
   prosta klasyfikacja - ludzie osi i aliantów

na początek: churchill, roosvelt, stalin, hitler, hirohito
