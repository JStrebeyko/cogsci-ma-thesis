{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# literature review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Seeing like AI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Why such a weak dection capabilits?\n",
    "\n",
    "> LLMs struggled with bias detection, achieving only 64% accuracy\n",
    "\n",
    "> Analysis of LLM rationales and errors suggested LLMs relied (sometimes to a fault) on simple heuristics like the presence of a highly subjective adjective.\n",
    "\n",
    "> Because LLMs are explicitly trained for natural language generation, they may be more effective at generation than detection.\n",
    "\n",
    "### models\n",
    "\n",
    "> We chose these models because GPT 4 is state-of-the-art, ChatGPT 3.5 is widely used, and Mistral-Medium is from an open-source developer.\n",
    "\n",
    "> high-recall but low-precision editing.\n",
    "\n",
    "> Models exhibited biases that persisted across prompts.\n",
    "\n",
    "> Understanding model idiosyncrasies is crucial. It is not clear that, when a new model appears on the scene, we will know what its priors are.\n",
    "\n",
    "> For model builders, we suggest several actionable steps to improve detection and generation abilities: Retrieval-augmented generation (e.g., incorporating Wikipedia Talk page conversations to simulate editors’ community knowledge) or multi-agent systems [15, 17] (simulating Wikipedia-style debates) may enhance detection capabilities. Fine-tuning models on expert feedback can increase alignment with domain specialists. Refining Constitutional AI to emphasize preserving original text may improve revision precision.\n",
    "\n",
    "### dataset\n",
    "\n",
    "> While NPOV is a set of multiple rules, the WNC consists of edits that violated a particular subset of NPOV—biased language (framing, epistemological, and demographic bias).\n",
    "\n",
    "> rules, the WNC consists of edits that violated a particular subset of NPOV—biased language (framing, epistemological, and demographic bias).\n",
    "\n",
    "> Number of edits for each topic. The data consists of rewrites of biased edits.\n",
    "\n",
    "even split between biased/non biased from each of article category (STEM, Culture, Geography....)\n",
    "\n",
    "### detection experiment\n",
    "\n",
    "> (...) conducted a multi-model prompt experiment (N = 5,348 annotations) where the task was to classify if a given Wikipedia edit was biased or neutral.\n",
    "\n",
    "> To steer models towards specific norms and values, there is a growing trend of stating high-level rules as prompts.\n",
    "> 3.2.1 Factor 1: Definitions Provided (minimal / NPOV / NPOV-Scoped, where additional bias information is provided)\n",
    "> 3.2.2 Factor 2: Examples Provided. (few-shot)\n",
    "\n",
    "### prompts engineering\n",
    "\n",
    "Used a single prompting technique and DSPY for insignificantly better results\n",
    "\n",
    "- perhaps these could be improved with better techniques?\n",
    "\n",
    "> (...) use with chain-of-thought (CoT) reasoning and DSPy’s ‘BootstrapFewShot’ module. In CoT, a model is instructed to reason step by step to get to its answer, which can improve performance [50].\n",
    "\n",
    "### further directions\n",
    "\n",
    "...bigger models\n",
    "\n",
    "> Notably, the largest model did the best. Perhaps even larger models would do better.\n",
    "\n",
    "...finetuning\n",
    "\n",
    "> Alternatively, low performance may be due to a more fundamental aspect of LLMs. For example, it may be that LLMs effectively ‘over-learn’ a notion (e.g., neutrality) from broad web corpora, and specializing this notion to a community’s norms requires changing a model’s parameters. In any case, our study suggests two future directions. First, fine-tuning may improve performance. However, fine-tuning may also risk overfitting [26].\n",
    "\n",
    "...rag, conversationr retrieval\n",
    "\n",
    "> Another approach involves more advanced use of retrieval-augmented-generation (RAG). Can incorporating conver sations from Wikipedia Talk pages or other Wikipedia data simulate the community knowledge that editors have?\n",
    "\n",
    "...multi-agent (mentioned in the context of generation, but who knows)\n",
    "\n",
    "> Perhaps a multi-agent system simulating ‘debates’ [15, 17] can better apply nuanced rules than an individual agent in isolation.\n",
    "\n",
    "> → Improving Detection: Retrieval-augmented generation (integrating Wikipedia’s Talk pages) that provides models with additional context for labels or multi-agent systems (simulating Wikipedia-style debates) may enhance norm violation detection capabilities.\n",
    "\n",
    "...automatic triggering of neutralizing edition logic by scanning discussion pages for traces of neutrality violitions\n",
    "\n",
    "> For instance, models could monitor Wikipedia Talk pages for comments suggesting NPOV violations. Based on automated triggers, these models could then offer neutralizations.\n",
    "\n",
    "... other types of NPOV violations\n",
    "\n",
    "... more models to be able to generalize better\n",
    "\n",
    "... better prompting\n",
    "\n",
    "> Fifth, no prompt experiment can rule out that some (untested) prompts may have worked better. Though we explored a diverse range of prompting strategies. Our main experiment centered around providing LLMs with the text of NPOV, which is a reasonable prompt strategy. We then systematically ablated examples and definitions. Subsequent experiments incorporated chain-of-thought reasoning, LLM-generated rationales, and LLM self-generated prompts. We encourage future research to explore additional techniques, such as more sophisticated few-shot example selection and retrieval-augmented generation.\n",
    "\n",
    "... wider context:\n",
    "\n",
    "> Sixth, our dataset is noisy: Edits appear without the full article context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Automatically Neutralizing Subjective Bias in Text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
